{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Micro Controller Benchmark\n",
    "\n",
    "Benchmark PyTorch Models and generate Compatibility list for micro controllers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "\n",
    "# %pip install torch torchaudio omegaconf soundfile numpy prettytable transformers pocketsphinx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Moritz\\anaconda3\\envs\\hws\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import wave\n",
    "import platform\n",
    "import torch\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from timeit import default_timer\n",
    "from typing import TypedDict, Callable\n",
    "from prettytable import PrettyTable\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from transformers import AutoProcessor, HubertForCTC\n",
    "from pocketsphinx import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types\n",
    "\n",
    "class Model(TypedDict):\n",
    "    name: str\n",
    "    num_inferred_samples: int\n",
    "    infer: Callable[[None], list[str]]\n",
    "    is_pytorch: bool\n",
    "\n",
    "class MicroController(TypedDict):\n",
    "    name: str\n",
    "    architecture: str\n",
    "    memory_mb: float\n",
    "    cpu_speed_ghz: float\n",
    "\n",
    "class ModelResults:\n",
    "    name: str\n",
    "    cpu_time_total_ms: float | None\n",
    "    mean_inference_time_ms: float\n",
    "    mean_memory_usage_mb: float\n",
    "    m_flops: float | None\n",
    "    samples_per_cpu_second: float | None\n",
    "    samples_per_inference_second: float\n",
    "\n",
    "class ModelMicroControllerResults:\n",
    "    model_name: str\n",
    "    estimated_cpu_time_total_ms: float | None\n",
    "    estimated_inference_time_ms: float\n",
    "    estimated_samples_per_cpu_second: float | None\n",
    "    estimated_samples_per_inference_second: float\n",
    "    memory_usage_percentage: float\n",
    "    compatible: bool\n",
    "\n",
    "class BenchmarkOptions:\n",
    "    cpu_speed_ghz: float\n",
    "    target_sampling_rate_khz: float\n",
    "    logfile_path: str\n",
    "\n",
    "Log = Callable[[str], None]\n",
    "\n",
    "CreateModel = Callable[[None], Model]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def byte_to_mb(byte: int) -> int:\n",
    "    return byte / (1024 ** 2)\n",
    "\n",
    "def create_logger(logifle_path: str) -> Log:\n",
    "    def __log(msg: str):\n",
    "        with open(logifle_path, 'a') as f:\n",
    "            f.write(f'{msg}\\n' )\n",
    "        print(msg)\n",
    "    return __log\n",
    "\n",
    "def log_hash_comment(content: str, log: Log):\n",
    "    content_str = f'# {content} #'\n",
    "    num_hashes = len(content_str)\n",
    "    hashes = '#' * num_hashes\n",
    "    log(\n",
    "        f'{hashes}\\n'\n",
    "        f'{content_str}\\n'\n",
    "        f'{hashes}\\n'\n",
    "        '\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_pytorch_model(model: Model, log: Log, row_limit = 5, iterations = 5) -> ModelResults:\n",
    "\n",
    "    #####################\n",
    "    # Instantiate Model #\n",
    "    #####################\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    model_name = model['name']\n",
    "    infer = model['infer']\n",
    "    num_samples = model['num_inferred_samples']\n",
    "\n",
    "    log_hash_comment(model_name, log)\n",
    "\n",
    "    ########################\n",
    "    # Run PyTorch Profiler #\n",
    "    ########################\n",
    "\n",
    "    with profile(activities=[ProfilerActivity.CPU], profile_memory=True, with_flops=True, record_shapes=True, with_stack=True) as prof:\n",
    "        with record_function(\"model_inference\"):\n",
    "            _out = model['infer']()\n",
    "\n",
    "    key_averages = prof.key_averages()\n",
    "    total_average = key_averages.total_average()\n",
    "\n",
    "    key_averages.table()\n",
    "\n",
    "    cpu_time_ms = total_average.cpu_time_total * 0.001\n",
    "    self_cpu_time_ms = total_average.self_cpu_time_total * 0.001\n",
    "    cpu_memory_usage_mb = byte_to_mb(total_average.cpu_memory_usage)\n",
    "    self_cpu_memory_usage_mb = byte_to_mb(total_average.self_cpu_memory_usage)\n",
    "    m_flops = total_average.flops * 0.000001\n",
    "\n",
    "    log(f'--- PyTorch Profile: {model_name} ---\\n')\n",
    "\n",
    "\n",
    "    log(\n",
    "        f'CPU time top {row_limit}\\n'\n",
    "        f'{key_averages.table(sort_by=\"cpu_time_total\", row_limit=row_limit)}'\n",
    "    )\n",
    "\n",
    "    log(\n",
    "        f'CPU memory usage top {row_limit}\\n'\n",
    "        f'{key_averages.table(sort_by=\"cpu_memory_usage\", row_limit=row_limit)}'\n",
    "    )\n",
    "\n",
    "    log(\n",
    "        f'MFLOPs top {row_limit}\\n'\n",
    "        f'{key_averages.table(sort_by=\"flops\", row_limit=row_limit)}'\n",
    "    )\n",
    "\n",
    "    log(\n",
    "        f'Total averages\\n'\n",
    "        f'CPU time total [ms]: {cpu_time_ms}\\n' \n",
    "        f'Self CPU time total [ms]: {self_cpu_time_ms}\\n'\n",
    "        f'CPU memory usage [Mb]: {cpu_memory_usage_mb}\\n'\n",
    "        f'Self CPU memory usage [Mb]: {self_cpu_memory_usage_mb}\\n'\n",
    "        f'MFLOPs: {m_flops}\\n'\n",
    "    )\n",
    "\n",
    "    ###############################\n",
    "    # Run psutil memory_full_info #\n",
    "    ###############################\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    process = psutil.Process(os.getpid())\n",
    "    psutil_mem_rss_b: list[int] = []\n",
    "    psutil_mem_uss_b: list[int] = []\n",
    "    for _ in range(iterations):\n",
    "        memory_info = process.memory_full_info()\n",
    "        psutil_mem_rss_b.append(memory_info.rss)\n",
    "        psutil_mem_uss_b.append(memory_info.uss)\n",
    "        _out = infer()\n",
    "\n",
    "    psutil_mem_rss_b = np.array(psutil_mem_rss_b)\n",
    "    psutil_mem_uss_b = np.array(psutil_mem_uss_b)\n",
    "    mean_psutil_mem_rss_b = np.mean(psutil_mem_rss_b)\n",
    "    mean_psutil_mem_uss_b = np.mean(psutil_mem_uss_b)\n",
    "    std_psutil_mem_rss = np.std(psutil_mem_rss_b)\n",
    "    std_psutil_mem_uss = np.std(psutil_mem_uss_b)\n",
    "\n",
    "    log(f'--- Psutil memory_full_info: {model_name} ---\\n')\n",
    "\n",
    "    log(\n",
    "        f'Over {iterations} iterations\\n'\n",
    "        f'Mean RSS [Mb]: {byte_to_mb(mean_psutil_mem_rss_b)}\\n'\n",
    "        f'Std RSS: {byte_to_mb(std_psutil_mem_rss)}\\n'\n",
    "        f'Mean USS [Mb]: {byte_to_mb(mean_psutil_mem_uss_b)}\\n'\n",
    "        f'Std USS: {byte_to_mb(std_psutil_mem_uss)}\\n'\n",
    "    )\n",
    "\n",
    "    ############################\n",
    "    # Run timeit default_timer #\n",
    "    ############################\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    inference_times_ms: list[int] = []\n",
    "    for _ in range(iterations):\n",
    "        start = default_timer()\n",
    "        _out = infer()\n",
    "        end = default_timer()\n",
    "        inference_times_ms.append((end - start) * 1000)\n",
    "    \n",
    "    inference_times_ms = np.array(inference_times_ms)\n",
    "    mean_inference_time_ms = np.mean(inference_times_ms)\n",
    "    std_inference_time = np.std(inference_times_ms)\n",
    "\n",
    "\n",
    "    log(f'--- Timeit default_timer: {model_name} ---\\n')\n",
    "\n",
    "    log(\n",
    "        f'Over {iterations} iterations\\n'\n",
    "        f'Mean inference time [ms]: {mean_inference_time_ms}\\n'\n",
    "        f'Std inference time: {std_inference_time}\\n'\n",
    "    )\n",
    "\n",
    "    #############################\n",
    "    # Calculate overall results #\n",
    "    #############################\n",
    "\n",
    "    mean_memory_usage_mb = byte_to_mb(np.mean(np.array([mean_psutil_mem_rss_b, mean_psutil_mem_uss_b])))  # cpu_memory_usage_mb\n",
    "    samples_per_cpu_second = num_samples / (self_cpu_time_ms * 0.001)\n",
    "    samples_per_inference_second = num_samples / (mean_inference_time_ms * 0.001)\n",
    "\n",
    "    results: ModelResults = {\n",
    "        'name': model_name,\n",
    "        'cpu_time_total_ms': self_cpu_time_ms,\n",
    "        'mean_inference_time_ms': mean_inference_time_ms,\n",
    "        'mean_memory_usage_mb': mean_memory_usage_mb,\n",
    "        'm_flops': m_flops,\n",
    "        'samples_per_cpu_second': samples_per_cpu_second,\n",
    "        'samples_per_inference_second': samples_per_inference_second,\n",
    "    }\n",
    "\n",
    "    log(f'--- Overall results: {model_name} ---\\n')\n",
    "\n",
    "    log(\n",
    "        f'Mean memory usage [Mb]: {mean_memory_usage_mb}\\n'\n",
    "        f'Samples per CPU second: {samples_per_cpu_second}\\n'\n",
    "        f'Samples per inference second: {samples_per_inference_second}\\n'\n",
    "        '\\n'\n",
    "    )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_unknown_model(model: Model, log: Log, iterations = 5) -> ModelResults:\n",
    "\n",
    "    #####################\n",
    "    # Instantiate Model #\n",
    "    #####################\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    model_name = model['name']\n",
    "    infer = model['infer']\n",
    "    num_samples = model['num_inferred_samples']\n",
    "\n",
    "    log_hash_comment(model_name, log)\n",
    "\n",
    "    ###############################\n",
    "    # Run psutil memory_full_info #\n",
    "    ###############################\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    process = psutil.Process(os.getpid())\n",
    "    psutil_mem_rss_b: list[int] = []\n",
    "    psutil_mem_uss_b: list[int] = []\n",
    "    for _ in range(iterations):\n",
    "        memory_info = process.memory_full_info()\n",
    "        psutil_mem_rss_b.append(memory_info.rss)\n",
    "        psutil_mem_uss_b.append(memory_info.uss)\n",
    "        _out = infer()\n",
    "\n",
    "    psutil_mem_rss_b = np.array(psutil_mem_rss_b)\n",
    "    psutil_mem_uss_b = np.array(psutil_mem_uss_b)\n",
    "    mean_psutil_mem_rss_b = np.mean(psutil_mem_rss_b)\n",
    "    mean_psutil_mem_uss_b = np.mean(psutil_mem_uss_b)\n",
    "    std_psutil_mem_rss = np.std(psutil_mem_rss_b)\n",
    "    std_psutil_mem_uss = np.std(psutil_mem_uss_b)\n",
    "\n",
    "    log(f'--- Psutil memory_full_info: {model_name} ---\\n')\n",
    "\n",
    "    log(\n",
    "        f'Over {iterations} iterations\\n'\n",
    "        f'Mean RSS [Mb]: {byte_to_mb(mean_psutil_mem_rss_b)}\\n'\n",
    "        f'Std RSS: {byte_to_mb(std_psutil_mem_rss)}\\n'\n",
    "        f'Mean USS [Mb]: {byte_to_mb(mean_psutil_mem_uss_b)}\\n'\n",
    "        f'Std USS: {byte_to_mb(std_psutil_mem_uss)}\\n'\n",
    "    )\n",
    "\n",
    "    ############################\n",
    "    # Run timeit default_timer #\n",
    "    ############################\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    inference_times_ms: list[int] = []\n",
    "    for _ in range(iterations):\n",
    "        start = default_timer()\n",
    "        _out = infer()\n",
    "        end = default_timer()\n",
    "        inference_times_ms.append((end - start) * 1000)\n",
    "    \n",
    "    inference_times_ms = np.array(inference_times_ms)\n",
    "    mean_inference_time_ms = np.mean(inference_times_ms)\n",
    "    std_inference_time = np.std(inference_times_ms)\n",
    "\n",
    "\n",
    "    log(f'--- Timeit default_timer: {model_name} ---\\n')\n",
    "\n",
    "    log(\n",
    "        f'Over {iterations} iterations\\n'\n",
    "        f'Mean inference time [ms]: {mean_inference_time_ms}\\n'\n",
    "        f'Std inference time: {std_inference_time}\\n'\n",
    "    )\n",
    "\n",
    "    #############################\n",
    "    # Calculate overall results #\n",
    "    #############################\n",
    "\n",
    "    mean_memory_usage_mb = byte_to_mb(np.mean(np.array([mean_psutil_mem_rss_b, mean_psutil_mem_uss_b])))\n",
    "    samples_per_inference_second = num_samples / (mean_inference_time_ms * 0.001)\n",
    "\n",
    "    results: ModelResults = {\n",
    "        'name': model_name,\n",
    "        'cpu_time_total_ms': None,\n",
    "        'mean_inference_time_ms': mean_inference_time_ms,\n",
    "        'mean_memory_usage_mb': mean_memory_usage_mb,\n",
    "        'm_flops': None,\n",
    "        'samples_per_cpu_second': None,\n",
    "        'samples_per_inference_second': samples_per_inference_second,\n",
    "    }\n",
    "\n",
    "    log(f'--- Overall results: {model_name} ---\\n')\n",
    "\n",
    "    log(\n",
    "        f'Mean memory usage [Mb]: {mean_memory_usage_mb}\\n'\n",
    "        f'Samples per inference second: {samples_per_inference_second}\\n'\n",
    "        '\\n'\n",
    "    )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(create_model: CreateModel, log: Log) -> ModelResults:\n",
    "    # Instantiate model\n",
    "    model = create_model()\n",
    "    is_pythorch_model = model['is_pytorch']\n",
    "    # Benchmark based on model type\n",
    "    if is_pythorch_model:\n",
    "        results = benchmark_pytorch_model(model, log)\n",
    "    else:\n",
    "        results = benchmark_unknown_model(model, log)\n",
    "    # Delete model from memory before running next benchmark\n",
    "    del model\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model_micro_controller(model_results: ModelResults, micro_controller: MicroController, options: BenchmarkOptions) -> ModelMicroControllerResults:\n",
    "    controller_cpu_speed_ghz = micro_controller['cpu_speed_ghz']\n",
    "    benchmark_cpu_speed_ghz = options['cpu_speed_ghz']\n",
    "    cpu_time_factor = benchmark_cpu_speed_ghz / controller_cpu_speed_ghz\n",
    "\n",
    "    if model_results['cpu_time_total_ms'] is not None:\n",
    "        estimated_cpu_time_total_ms = model_results['cpu_time_total_ms'] * cpu_time_factor\n",
    "    else:\n",
    "        estimated_cpu_time_total_ms = None\n",
    "    estimated_inference_time_ms = model_results['mean_inference_time_ms'] * cpu_time_factor\n",
    "\n",
    "    if model_results['samples_per_cpu_second'] is not None:\n",
    "        estimated_samples_per_cpu_second = model_results['samples_per_cpu_second'] / cpu_time_factor\n",
    "    else:\n",
    "        estimated_samples_per_cpu_second = None\n",
    "    estimated_samples_per_inference_second = model_results['samples_per_inference_second'] / cpu_time_factor\n",
    "\n",
    "    memory_usage_percentage = (model_results['mean_memory_usage_mb'] / micro_controller['memory_mb']) * 100\n",
    "\n",
    "    target_sampling_rate_hz = options['target_sampling_rate_khz'] * 1000\n",
    "\n",
    "    compatible = (\n",
    "        # estimated_samples_per_cpu_second >= target_sampling_rate_hz \n",
    "        estimated_samples_per_inference_second >= target_sampling_rate_hz\n",
    "        and memory_usage_percentage <= 100\n",
    "    )\n",
    "\n",
    "    results: ModelMicroControllerResults = {\n",
    "        'model_name': model_results['name'],\n",
    "        'estimated_cpu_time_total_ms': estimated_cpu_time_total_ms,\n",
    "        'estimated_inference_time_ms': estimated_inference_time_ms,\n",
    "        'estimated_samples_per_cpu_second': estimated_samples_per_cpu_second,\n",
    "        'estimated_samples_per_inference_second': estimated_samples_per_inference_second,\n",
    "        'memory_usage_percentage': memory_usage_percentage,\n",
    "        'compatible': compatible,\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(create_models: list[CreateModel], micro_controllers: list[MicroController], options: BenchmarkOptions):\n",
    "    ##################\n",
    "    # Create logfile #\n",
    "    ##################\n",
    "\n",
    "    logfile_path = options['logfile_path']\n",
    "    f = open(logfile_path, 'w')\n",
    "    f.close()\n",
    "\n",
    "    log = create_logger(logfile_path)\n",
    "\n",
    "    ###################\n",
    "    # Log system info #\n",
    "    ###################\n",
    "\n",
    "    machine = platform.machine()\n",
    "    system = platform.system()\n",
    "    version = platform.version()\n",
    "    processor = platform.processor()\n",
    "    ram = psutil.virtual_memory().total / (1024.0 **3)\n",
    "\n",
    "    log(\n",
    "        f'--- Running benchmark on ---\\n'\n",
    "        '\\n'\n",
    "        f'Arch: {machine}\\n'\n",
    "        f'Platform: {system} {version}\\n'\n",
    "        f'CPU: {processor}, {options[\"cpu_speed_ghz\"]} GHz\\n'\n",
    "        f'RAM: {ram} GB\\n'\n",
    "    )\n",
    "\n",
    "\n",
    "    ###############################\n",
    "    # Benchmark individual models #\n",
    "    ###############################\n",
    "\n",
    "    log(\n",
    "        f'--- Benchmarking {len(create_models)} models ---\\n'\n",
    "        '\\n'\n",
    "    )\n",
    "    models_results = [benchmark_model(create_model, log) for create_model in create_models]\n",
    "\n",
    "    ##########################################\n",
    "    # Benchmark models for micro controllers #\n",
    "    ##########################################\n",
    "\n",
    "    log(\n",
    "        f'\\n--- Benchmarking {len(create_models)} models for {len(micro_controllers)} micro controllers ---\\n'\n",
    "        '\\n'\n",
    "    )\n",
    "\n",
    "    for micro_controller in micro_controllers:\n",
    "        log_hash_comment(micro_controller['name'], log)\n",
    "        log(\n",
    "            'Info\\n'\n",
    "            f'Architecture: {micro_controller[\"architecture\"]}\\n'\n",
    "            f'Memory [Mb]: {micro_controller[\"memory_mb\"]}\\n'\n",
    "            f'CPU speed [GHz]: {micro_controller[\"cpu_speed_ghz\"]}\\n'\n",
    "        )\n",
    "        table = PrettyTable(\n",
    "            ['Model', 'Estimated CPU time [ms]', 'Estimated inference time [ms]', 'Estimated samples per CPU second', 'Estimated samples per inference second', 'Memory usage %', 'COMPATIBLE']\n",
    "        )\n",
    "        results = [benchmark_model_micro_controller(model_result, micro_controller, options).values() for model_result in models_results]\n",
    "        table.add_rows(results)\n",
    "        log(table.get_string())\n",
    "        log('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret PyTorch Profiler results\n",
    "\n",
    "References:\n",
    "- [Recipe](https://h-huang.github.io/tutorials/recipes/recipes/profiler_recipe.html)\n",
    "\n",
    "#### CPU time\n",
    "\n",
    "CPU time vs self CPU time: operators can call other operators -> self cpu time excludes time spent in children operator calls, while total cpu time includes it\n",
    "\n",
    "#### Memory usage\n",
    "\n",
    "- Shows amount of memory used by the model’s tensors:\n",
    "- That was allocated (or released) during the execution of the model’s operators\n",
    "\n",
    "Self memory: corresponds to the memory allocated (released) by the operator, excluding the children calls to the other operators"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Silero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Moritz/.cache\\torch\\hub\\snakers4_silero-models_master\n",
      "100%|██████████| 0.99M/0.99M [00:00<00:00, 1.19MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Input statistics ---\n",
      "Sampling rate: 48000 Hz\n",
      "Number of samples: 518400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# Use Silero utils to download model and test file #\n",
    "####################################################\n",
    "\n",
    "# Always use CPU (simulate run on micro controller)\n",
    "device = torch.device('cpu')  \n",
    "\n",
    "# Download model, decoder and utils\n",
    "model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models',\n",
    "                                       model='silero_stt',\n",
    "                                       language='en', # also available 'de', 'es'\n",
    "                                       device=device\n",
    ")\n",
    "(read_batch, split_into_batches, _ , prepare_model_input) = utils  # see function signature for details\n",
    "\n",
    "# Download a single test file, any format compatible with TorchAudio (soundfile backend)\n",
    "torch.hub.download_url_to_file('https://opus-codec.org/static/examples/samples/speech_orig.wav',\n",
    "                            dst ='speech_orig.wav', progress=True)\n",
    "test_files = glob('speech_orig.wav')\n",
    "\n",
    "######################################\n",
    "# Get number of samples in test data #\n",
    "######################################\n",
    "\n",
    "audio_file = wave.open(test_files[0], 'r')\n",
    "sampling_rate = audio_file.getframerate()\n",
    "num_samples = audio_file.getnframes()\n",
    "\n",
    "print(\n",
    "    f'\\n--- Input statistics ---\\n'\n",
    "    f'Sampling rate: {sampling_rate} Hz\\n'\n",
    "    f'Number of samples: {num_samples}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"the boch canoe slit on the smooth planks blew the sheet to the dark blue background it's easy to tell a depth of a well four hours of steady work faced us\"]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# Create SILERO model #\n",
    "#######################\n",
    "\n",
    "def create_silero_model():\n",
    "    # Prepare input data\n",
    "    batches = split_into_batches(test_files, batch_size=10)\n",
    "\n",
    "    #################################\n",
    "    # Create model for benchmarking #\n",
    "    #################################\n",
    "\n",
    "    def infer_silero():\n",
    "        batch_tensor = read_batch(batches[0])\n",
    "        input = prepare_model_input(batch_tensor, device=device)\n",
    "        output = model(input)\n",
    "        return [decoder(example.cpu()) for example in output]\n",
    "\n",
    "    silero_model: Model = {\n",
    "        'name': 'Silero',\n",
    "        'num_inferred_samples': num_samples,\n",
    "        'infer': infer_silero,\n",
    "        'is_pytorch': True,\n",
    "    }\n",
    "\n",
    "    return silero_model\n",
    "\n",
    "#######################################\n",
    "# Run model inference and log results #\n",
    "#######################################\n",
    "\n",
    "silero_model = create_silero_model()\n",
    "transcription = silero_model['infer']()\n",
    "print(transcription)\n",
    "\n",
    "del silero_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antlr4-python3-runtime==4.9.3\n",
      "asttokens @ file:///home/conda/feedstock_root/build_artifacts/asttokens_1670263926556/work\n",
      "backcall @ file:///home/conda/feedstock_root/build_artifacts/backcall_1592338393461/work\n",
      "backports.functools-lru-cache @ file:///home/conda/feedstock_root/build_artifacts/backports.functools_lru_cache_1618230623929/work\n",
      "certifi==2023.5.7\n",
      "cffi==1.15.1\n",
      "charset-normalizer==3.1.0\n",
      "colorama @ file:///home/conda/feedstock_root/build_artifacts/colorama_1666700638685/work\n",
      "contourpy==1.0.7\n",
      "cycler==0.11.0\n",
      "debugpy @ file:///C:/ci_310/debugpy_1642079916595/work\n",
      "decorator @ file:///home/conda/feedstock_root/build_artifacts/decorator_1641555617451/work\n",
      "executing @ file:///home/conda/feedstock_root/build_artifacts/executing_1667317341051/work\n",
      "filelock==3.12.0\n",
      "flops-profiler==0.1.2\n",
      "fonttools==4.39.4\n",
      "fsspec==2023.5.0\n",
      "huggingface-hub==0.15.1\n",
      "idna==3.4\n",
      "importlib-metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1682176699712/work\n",
      "ipykernel @ file:///D:/bld/ipykernel_1655369313836/work\n",
      "ipython @ file:///D:/bld/ipython_1685727936079/work\n",
      "jedi @ file:///home/conda/feedstock_root/build_artifacts/jedi_1669134318875/work\n",
      "Jinja2==3.1.2\n",
      "jupyter_client @ file:///home/conda/feedstock_root/build_artifacts/jupyter_client_1681432441054/work\n",
      "jupyter_core @ file:///D:/bld/jupyter_core_1678994400417/work\n",
      "kiwisolver==1.4.4\n",
      "MarkupSafe==2.1.3\n",
      "matplotlib==3.7.1\n",
      "matplotlib-inline @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-inline_1660814786464/work\n",
      "mpmath==1.3.0\n",
      "nest-asyncio @ file:///home/conda/feedstock_root/build_artifacts/nest-asyncio_1664684991461/work\n",
      "networkx==3.1\n",
      "numpy==1.24.3\n",
      "omegaconf==2.3.0\n",
      "packaging @ file:///home/conda/feedstock_root/build_artifacts/packaging_1681337016113/work\n",
      "parso @ file:///home/conda/feedstock_root/build_artifacts/parso_1638334955874/work\n",
      "pickleshare @ file:///home/conda/feedstock_root/build_artifacts/pickleshare_1602536217715/work\n",
      "Pillow==9.5.0\n",
      "platformdirs @ file:///home/conda/feedstock_root/build_artifacts/platformdirs_1683850015520/work\n",
      "pocketsphinx==5.0.1\n",
      "prettytable==3.7.0\n",
      "prompt-toolkit @ file:///home/conda/feedstock_root/build_artifacts/prompt-toolkit_1677600924538/work\n",
      "psutil @ file:///C:/Windows/Temp/abs_b2c2fd7f-9fd5-4756-95ea-8aed74d0039flsd9qufz/croots/recipe/psutil_1656431277748/work\n",
      "pure-eval @ file:///home/conda/feedstock_root/build_artifacts/pure_eval_1642875951954/work\n",
      "pycparser==2.21\n",
      "Pygments @ file:///home/conda/feedstock_root/build_artifacts/pygments_1681904169130/work\n",
      "pyparsing==3.0.9\n",
      "python-dateutil @ file:///home/conda/feedstock_root/build_artifacts/python-dateutil_1626286286081/work\n",
      "pywin32==305.1\n",
      "PyYAML==6.0\n",
      "pyzmq @ file:///C:/b/abs_8b16zbmf46/croot/pyzmq_1682697651374/work\n",
      "regex==2023.6.3\n",
      "requests==2.31.0\n",
      "safetensors==0.3.1\n",
      "six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work\n",
      "sounddevice==0.4.6\n",
      "soundfile==0.12.1\n",
      "stack-data @ file:///home/conda/feedstock_root/build_artifacts/stack_data_1669632077133/work\n",
      "sympy==1.12\n",
      "tokenizers==0.13.3\n",
      "torch==2.0.1\n",
      "torchaudio==2.0.2\n",
      "tornado @ file:///D:/bld/tornado_1656937966227/work\n",
      "tqdm==4.65.0\n",
      "traitlets @ file:///home/conda/feedstock_root/build_artifacts/traitlets_1675110562325/work\n",
      "transformers==4.30.0\n",
      "typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1685704949284/work\n",
      "urllib3==2.0.3\n",
      "wcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1673864653149/work\n",
      "zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1677313463193/work\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip freeze"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PocketSphinx\n",
    "\n",
    "Reference: [PyPI](https://pypi.org/project/pocketsphinx/), [Example](https://github.com/cmusphinx/pocketsphinx/blob/master/examples/simple.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the pitch kinnear slipped on the snooze planks linda say to the doc the loop act grounds it's easy to tell him that the well for allies and steady work face death\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Create PocketSphinx model #\n",
    "#############################\n",
    "\n",
    "def create_pocket_sphinx_model():\n",
    "\n",
    "    # Configure decoder\n",
    "    decoder = Decoder(samprate=sampling_rate)\n",
    "\n",
    "    #################################\n",
    "    # Create model for benchmarking #\n",
    "    #################################\n",
    "\n",
    "    def infer_pocket_sphinx():\n",
    "        with wave.open('speech_orig.wav', 'rb') as audio:\n",
    "            decoder.start_utt()\n",
    "            decoder.process_raw(audio.getfp().read(), full_utt=True)\n",
    "            decoder.end_utt()\n",
    "            return decoder.hyp().hypstr\n",
    "       \n",
    "\n",
    "    pocket_sphinx_model: Model = {\n",
    "        'name': 'PocketSphinx',\n",
    "        'num_inferred_samples': num_samples,\n",
    "        'infer': infer_pocket_sphinx,\n",
    "        'is_pytorch': False,\n",
    "    }\n",
    "\n",
    "    return pocket_sphinx_model\n",
    "\n",
    "#######################################\n",
    "# Run model inference and log results #\n",
    "#######################################\n",
    "\n",
    "pocket_sphinx_model = create_pocket_sphinx_model()\n",
    "transcription = pocket_sphinx_model['infer']()\n",
    "print(transcription)\n",
    "\n",
    "del pocket_sphinx_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuBERT\n",
    "\n",
    "Reference: [Hugging Face](https://huggingface.co/docs/transformers/model_doc/hubert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"THE BIRCH CANOE SLID ON THE SMOOTH PLANKS GLUE THE SHEET TO THE DARK BLUE BACKGROUND IT'S EASY TO TELL THE DEPTH OF A WELL FOUR HOURS OF STEADY WORK FACED US\"]\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# Create HuBERT model #\n",
    "#######################\n",
    "\n",
    "def create_hubert_model():\n",
    "    # Download model\n",
    "    model = HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "    processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "\n",
    "    # Prepare inputs\n",
    "    batches = split_into_batches(test_files, batch_size=10)\n",
    "\n",
    "    #################################\n",
    "    # Create model for benchmarking #\n",
    "    #################################\n",
    "\n",
    "    def infer_hubert():\n",
    "        inputs = processor(read_batch(batches[0])[0], sampling_rate=16_000, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        return processor.batch_decode(predicted_ids)\n",
    "\n",
    "    hubert_model: Model = {\n",
    "        'name': 'HuBERT',\n",
    "        'num_inferred_samples': num_samples,\n",
    "        'infer': infer_hubert,\n",
    "        'is_pytorch': True,\n",
    "    }\n",
    "\n",
    "    return hubert_model\n",
    "\n",
    "#######################################\n",
    "# Run model inference and log results #\n",
    "#######################################\n",
    "\n",
    "pocket_sphinx_model = create_hubert_model()\n",
    "transcription = pocket_sphinx_model['infer']()\n",
    "print(transcription)\n",
    "\n",
    "del pocket_sphinx_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Micro Controllers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Create micro controller for benchmarking #\n",
    "############################################\n",
    "\n",
    "esp32: MicroController = {\n",
    "    'name': 'ESP32',\n",
    "    'architecture': '32-bit RISC-V',\n",
    "    'cpu_speed_ghz': 0.24,\n",
    "    'memory_mb': 0.23,\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raspberry Pi Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Create micro controller for benchmarking #\n",
    "############################################\n",
    "\n",
    "pi_zero: MicroController = {\n",
    "    'name': 'Raspberry Pi Zero',\n",
    "    'architecture': '32-bit ARM',\n",
    "    'cpu_speed_ghz': 1,\n",
    "    'memory_mb': 512,\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeagleBone Black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Create micro controller for benchmarking #\n",
    "############################################\n",
    "\n",
    "beagle_bone_black: MicroController = {\n",
    "    'name': 'BeagleBone Black',\n",
    "    'architecture': 'ARM Cortex-A8',\n",
    "    'cpu_speed_ghz': 1,\n",
    "    'memory_mb': 512,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raspberry Pi 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Create micro controller for benchmarking #\n",
    "############################################\n",
    "\n",
    "pi_3_b: MicroController = {\n",
    "    'name': 'Raspberry Pi 3 B',\n",
    "    'architecture': '64-bit ARM',\n",
    "    'cpu_speed_ghz': 1.2,\n",
    "    'memory_mb': 1000,\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raspberry Pi 4 Model B 4Go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# Create micro controller for benchmarking #\n",
    "############################################\n",
    "\n",
    "pi_4_b: MicroController = {\n",
    "    'name': 'Raspberry Pi 4 Model B 4Go',\n",
    "    'architecture': '64-bit ARM',\n",
    "    'cpu_speed_ghz': 1.5,\n",
    "    'memory_mb': 4000,\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running benchmark on ---\n",
      "\n",
      "Arch: AMD64\n",
      "Platform: Windows 10.0.22621\n",
      "CPU: AMD64 Family 23 Model 8 Stepping 2, AuthenticAMD, 3.7 GHz\n",
      "RAM: 15.951824188232422 GB\n",
      "\n",
      "--- Benchmarking 3 models ---\n",
      "\n",
      "\n",
      "################\n",
      "# PocketSphinx #\n",
      "################\n",
      "\n",
      "\n",
      "--- Psutil memory_full_info: PocketSphinx ---\n",
      "\n",
      "Over 5 iterations\n",
      "Mean RSS [Mb]: 531.29765625\n",
      "Std RSS: 5.94583573995148\n",
      "Mean USS [Mb]: 478.52578125\n",
      "Std USS: 5.01000561376931\n",
      "\n",
      "--- Timeit default_timer: PocketSphinx ---\n",
      "\n",
      "Over 5 iterations\n",
      "Mean inference time [ms]: 2146.356899966486\n",
      "Std inference time: 93.1672998273074\n",
      "\n",
      "--- Overall results: PocketSphinx ---\n",
      "\n",
      "Mean memory usage [Mb]: 504.91171875000003\n",
      "Samples per inference second: 241525.5356684131\n",
      "\n",
      "\n",
      "##########\n",
      "# Silero #\n",
      "##########\n",
      "\n",
      "\n",
      "--- PyTorch Profile: Silero ---\n",
      "\n",
      "CPU time top 5\n",
      "------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Total MFLOPs  \n",
      "------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "               model_inference        66.12%     509.612ms       100.00%     770.690ms     770.690ms           0 b      -3.82 Mb             1            --  \n",
      "                       forward         2.41%      18.590ms        32.19%     248.086ms     248.086ms     530.72 Kb     -57.35 Mb             1            --  \n",
      "                  aten::linear         0.62%       4.776ms        13.98%     107.728ms       1.683ms      31.80 Mb      -6.36 Mb            64            --  \n",
      "                   aten::addmm        12.35%      95.154ms        13.02%     100.359ms       1.568ms      38.16 Mb      38.16 Mb            64     10242.490  \n",
      "                  aten::conv1d         0.20%       1.524ms         6.56%      50.590ms       2.024ms      11.22 Mb     -10.57 Mb            25            --  \n",
      "------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 770.690ms\n",
      "\n",
      "CPU memory usage top 5\n",
      "------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Total MFLOPs  \n",
      "------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                   aten::empty         0.11%     817.000us         0.11%     817.000us       3.229us      77.49 Mb      77.49 Mb           253            --  \n",
      "              aten::empty_like         0.15%       1.135ms         0.19%       1.447ms      14.616us      40.00 Mb       1.59 Mb            99            --  \n",
      "                   aten::clone         0.29%       2.262ms         1.20%       9.269ms      94.582us      39.34 Mb      -1.06 Mb            98            --  \n",
      "                   aten::addmm        12.35%      95.154ms        13.02%     100.359ms       1.568ms      38.16 Mb      38.16 Mb            64     10242.490  \n",
      "                  aten::linear         0.62%       4.776ms        13.98%     107.728ms       1.683ms      31.80 Mb      -6.36 Mb            64            --  \n",
      "------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 770.690ms\n",
      "\n",
      "MFLOPs top 5\n",
      "------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                          Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Total MFLOPs  \n",
      "------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                   aten::addmm        12.35%      95.154ms        13.02%     100.359ms       1.568ms      38.16 Mb      38.16 Mb            64     10242.490  \n",
      "                     aten::bmm         2.10%      16.163ms         2.10%      16.163ms     505.094us      11.97 Mb      11.97 Mb            32      1506.296  \n",
      "                     aten::add         0.63%       4.871ms         0.63%       4.884ms      66.000us     -13.28 Mb     -13.28 Mb            74         9.058  \n",
      "                     aten::mul         0.42%       3.207ms         0.43%       3.280ms      46.857us      14.30 Mb      14.30 Mb            70         7.188  \n",
      "               model_inference        66.12%     509.612ms       100.00%     770.690ms     770.690ms           0 b      -3.82 Mb             1            --  \n",
      "------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 770.690ms\n",
      "\n",
      "Total averages\n",
      "CPU time total [ms]: 1582.536\n",
      "Self CPU time total [ms]: 770.69\n",
      "CPU memory usage [Mb]: 365.2435531616211\n",
      "Self CPU memory usage [Mb]: 0.0\n",
      "MFLOPs: 11765.032775\n",
      "\n",
      "--- Psutil memory_full_info: Silero ---\n",
      "\n",
      "Over 5 iterations\n",
      "Mean RSS [Mb]: 481.540625\n",
      "Std RSS: 2.588496656634165\n",
      "Mean USS [Mb]: 430.83046875\n",
      "Std USS: 2.3924875301725717\n",
      "\n",
      "--- Timeit default_timer: Silero ---\n",
      "\n",
      "Over 5 iterations\n",
      "Mean inference time [ms]: 209.60385999642313\n",
      "Std inference time: 1.9393525218418184\n",
      "\n",
      "--- Overall results: Silero ---\n",
      "\n",
      "Mean memory usage [Mb]: 456.185546875\n",
      "Samples per CPU second: 672643.9943427318\n",
      "Samples per inference second: 2473236.89558411\n",
      "\n",
      "\n",
      "##########\n",
      "# HuBERT #\n",
      "##########\n",
      "\n",
      "\n",
      "--- PyTorch Profile: HuBERT ---\n",
      "\n",
      "CPU time top 5\n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Total MFLOPs  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                 model_inference         6.38%     277.567ms       100.00%        4.350s        4.350s      32.00 Mb      -2.80 Gb             1            --  \n",
      "                    aten::linear         0.14%       6.192ms        61.44%        2.673s      18.307ms     456.95 Mb           0 b           146            --  \n",
      "                     aten::addmm        59.98%        2.609s        61.16%        2.661s      18.223ms     456.95 Mb     456.95 Mb           146    326145.606  \n",
      "                    aten::conv1d         0.00%      88.000us        12.20%     530.788ms      58.976ms     136.70 Mb           0 b             9            --  \n",
      "               aten::convolution         0.00%     134.000us        12.20%     530.700ms      58.967ms     136.70 Mb           0 b             9            --  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.350s\n",
      "\n",
      "CPU memory usage top 5\n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Total MFLOPs  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     aten::empty         0.10%       4.438ms         0.10%       4.438ms      14.362us     881.02 Mb     881.02 Mb           309            --  \n",
      "                       aten::add         1.57%      68.231ms         1.57%      68.239ms     842.457us     528.74 Mb     528.74 Mb            81       138.605  \n",
      "                aten::empty_like         0.01%     562.000us         0.08%       3.414ms      30.212us     503.05 Mb       4.21 Mb           113            --  \n",
      "                       aten::bmm         6.22%     270.474ms         6.22%     270.485ms       5.635ms     476.10 Mb     476.10 Mb            48     28559.376  \n",
      "                     aten::clone         0.02%     979.000us         3.59%     156.042ms       1.419ms     471.04 Mb      -4.21 Mb           110            --  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.350s\n",
      "\n",
      "MFLOPs top 5\n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  Total MFLOPs  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                     aten::addmm        59.98%        2.609s        61.16%        2.661s      18.223ms     456.95 Mb     456.95 Mb           146    326145.606  \n",
      "                       aten::bmm         6.22%     270.474ms         6.22%     270.485ms       5.635ms     476.10 Mb     476.10 Mb            48     28559.376  \n",
      "                       aten::add         1.57%      68.231ms         1.57%      68.239ms     842.457us     528.74 Mb     528.74 Mb            81       138.605  \n",
      "                       aten::mul         0.20%       8.905ms         0.22%       9.545ms     353.519us      50.53 Mb      50.53 Mb            27        13.247  \n",
      "                 model_inference         6.38%     277.567ms       100.00%        4.350s        4.350s      32.00 Mb      -2.80 Gb             1            --  \n",
      "--------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.350s\n",
      "\n",
      "Total averages\n",
      "CPU time total [ms]: 13438.288\n",
      "Self CPU time total [ms]: 4350.451\n",
      "CPU memory usage [Mb]: 6621.4806842803955\n",
      "Self CPU memory usage [Mb]: 32.0\n",
      "MFLOPs: 354856.83403699996\n",
      "\n",
      "--- Psutil memory_full_info: HuBERT ---\n",
      "\n",
      "Over 5 iterations\n",
      "Mean RSS [Mb]: 1716.30078125\n",
      "Std RSS: 1.6601590073506514\n",
      "Mean USS [Mb]: 1668.44921875\n",
      "Std USS: 1.644534033548713\n",
      "\n",
      "--- Timeit default_timer: HuBERT ---\n",
      "\n",
      "Over 5 iterations\n",
      "Mean inference time [ms]: 4386.372960009612\n",
      "Std inference time: 107.68521972077873\n",
      "\n",
      "--- Overall results: HuBERT ---\n",
      "\n",
      "Mean memory usage [Mb]: 1692.375\n",
      "Samples per CPU second: 119160.0594972797\n",
      "Samples per inference second: 118184.20474643452\n",
      "\n",
      "\n",
      "\n",
      "--- Benchmarking 3 models for 5 micro controllers ---\n",
      "\n",
      "\n",
      "#########\n",
      "# ESP32 #\n",
      "#########\n",
      "\n",
      "\n",
      "Info\n",
      "Architecture: 32-bit RISC-V\n",
      "Memory [Mb]: 0.23\n",
      "CPU speed [GHz]: 0.24\n",
      "\n",
      "+--------------+-------------------------+-------------------------------+----------------------------------+----------------------------------------+--------------------+------------+\n",
      "|    Model     | Estimated CPU time [ms] | Estimated inference time [ms] | Estimated samples per CPU second | Estimated samples per inference second |   Memory usage %   | COMPATIBLE |\n",
      "+--------------+-------------------------+-------------------------------+----------------------------------+----------------------------------------+--------------------+------------+\n",
      "| PocketSphinx |           None          |       33089.66887448332       |               None               |           15666.521232545712           | 219526.83423913046 |   False    |\n",
      "|    Silero    |    11881.470833333335   |       3231.3928416115236      |        43630.96179520422         |           160426.1770108612            | 198341.5421195652  |   False    |\n",
      "|    HuBERT    |    67069.45291666668    |       67623.24980014819       |        7729.301156580304         |           7666.002470038996            | 735815.2173913043  |   False    |\n",
      "+--------------+-------------------------+-------------------------------+----------------------------------+----------------------------------------+--------------------+------------+\n",
      "\n",
      "\n",
      "#####################\n",
      "# Raspberry Pi Zero #\n",
      "#####################\n",
      "\n",
      "\n",
      "Info\n",
      "Architecture: 32-bit ARM\n",
      "Memory [Mb]: 512\n",
      "CPU speed [GHz]: 1\n",
      "\n",
      "+--------------+-------------------------+-------------------------------+----------------------------------+----------------------------------------+-------------------+------------+\n",
      "|    Model     | Estimated CPU time [ms] | Estimated inference time [ms] | Estimated samples per CPU second | Estimated samples per inference second |   Memory usage %  | COMPATIBLE |\n",
      "+--------------+-------------------------+-------------------------------+----------------------------------+----------------------------------------+-------------------+------------+\n",
      "| PocketSphinx |           None          |       7941.520529875997       |               None               |           65277.171802273806           | 98.61557006835938 |    True    |\n",
      "|    Silero    |    2851.5530000000003   |       775.5342819867656       |        181795.67414668426        |           668442.4042119216            | 89.09873962402344 |    True    |\n",
      "|    HuBERT    |        16096.6687       |       16229.579952035565      |        32205.421485751267        |           31941.676958495817           |   330.5419921875  |   False    |\n",
      "+--------------+-------------------------+-------------------------------+----------------------------------+----------------------------------------+-------------------+------------+\n",
      "\n",
      "\n",
      "####################\n",
      "# BeagleBone Black #\n",
      "####################\n",
      "\n",
      "\n",
      "Info\n",
      "Architecture: ARM Cortex-A8\n",
      "Memory [Mb]: 512\n",
      "CPU speed [GHz]: 1\n",
      "\n",
      "+--------------+-------------------------+-------------------------------+----------------------------------+----------------------------------------+-------------------+------------+\n",
      "|    Model     | Estimated CPU time [ms] | Estimated inference time [ms] | Estimated samples per CPU second | Estimated samples per inference second |   Memory usage %  | COMPATIBLE |\n",
      "+--------------+-------------------------+-------------------------------+----------------------------------+----------------------------------------+-------------------+------------+\n",
      "| PocketSphinx |           None          |       7941.520529875997       |               None               |           65277.171802273806           | 98.61557006835938 |    True    |\n",
      "|    Silero    |    2851.5530000000003   |       775.5342819867656       |        181795.67414668426        |           668442.4042119216            | 89.09873962402344 |    True    |\n",
      "|    HuBERT    |        16096.6687       |       16229.579952035565      |        32205.421485751267        |           31941.676958495817           |   330.5419921875  |   False    |\n",
      "+--------------+-------------------------+-------------------------------+----------------------------------+----------------------------------------+-------------------+------------+\n",
      "\n",
      "\n",
      "####################\n",
      "# Raspberry Pi 3 B #\n",
      "####################\n",
      "\n",
      "\n",
      "Info\n",
      "Architecture: 64-bit ARM\n",
      "Memory [Mb]: 1000\n",
      "CPU speed [GHz]: 1.2\n",
      "\n",
      "+--------------+-------------------------+-------------------------------+----------------------------------+----------------------------------------+--------------------+------------+\n",
      "|    Model     | Estimated CPU time [ms] | Estimated inference time [ms] | Estimated samples per CPU second | Estimated samples per inference second |   Memory usage %   | COMPATIBLE |\n",
      "+--------------+-------------------------+-------------------------------+----------------------------------+----------------------------------------+--------------------+------------+\n",
      "| PocketSphinx |           None          |       6617.9337748966645      |               None               |           78332.60616272857            | 50.491171875000006 |    True    |\n",
      "|    Silero    |    2376.294166666667    |       646.2785683223046       |        218154.8089760211         |            802130.885054306            |   45.6185546875    |    True    |\n",
      "|    HuBERT    |    13413.890583333334   |       13524.649960029637      |        38646.50578290152         |           38330.01235019498            | 169.23749999999998 |   False    |\n",
      "+--------------+-------------------------+-------------------------------+----------------------------------+----------------------------------------+--------------------+------------+\n",
      "\n",
      "\n",
      "##############################\n",
      "# Raspberry Pi 4 Model B 4Go #\n",
      "##############################\n",
      "\n",
      "\n",
      "Info\n",
      "Architecture: 64-bit ARM\n",
      "Memory [Mb]: 4000\n",
      "CPU speed [GHz]: 1.5\n",
      "\n",
      "+--------------+-------------------------+-------------------------------+----------------------------------+----------------------------------------+--------------------+------------+\n",
      "|    Model     | Estimated CPU time [ms] | Estimated inference time [ms] | Estimated samples per CPU second | Estimated samples per inference second |   Memory usage %   | COMPATIBLE |\n",
      "+--------------+-------------------------+-------------------------------+----------------------------------+----------------------------------------+--------------------+------------+\n",
      "| PocketSphinx |           None          |       5294.347019917332       |               None               |           97915.75770341071            | 12.622792968750002 |    True    |\n",
      "|    Silero    |    1901.0353333333335   |       517.0228546578437       |        272693.5112200264         |           1002663.6063178824           |  11.404638671875   |    True    |\n",
      "|    HuBERT    |    10731.112466666667   |       10819.71996802371       |         48308.1322286269         |           47912.51543774372            | 42.309374999999996 |    True    |\n",
      "+--------------+-------------------------+-------------------------------+----------------------------------+----------------------------------------+--------------------+------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "# Register models and micro controllers #\n",
    "#########################################\n",
    "\n",
    "models: list[Model] = [create_pocket_sphinx_model, create_silero_model, create_hubert_model]\n",
    "\n",
    "micro_controllers: list[MicroController] = [esp32, pi_zero, beagle_bone_black, pi_3_b, pi_4_b]\n",
    "\n",
    "#########################\n",
    "# Set benchmark options #\n",
    "#########################\n",
    "\n",
    "benchmark_options: BenchmarkOptions = {\n",
    "    'cpu_speed_ghz': 3.7,\n",
    "    'target_sampling_rate_khz': 16,\n",
    "    'logfile_path': 'benchmark.txt'\n",
    "}\n",
    "\n",
    "#################\n",
    "# Run benchmark #\n",
    "#################\n",
    "\n",
    "benchmark(models, micro_controllers, benchmark_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
